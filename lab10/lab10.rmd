---
title: "Model Building & Evaluation (lab10)"
author: "Brian Nguyen"
date: "4/16/2021"
output:
  html_notebook: default
  html_document: default
subtitle: BSAD 343H, Business Analytics, Spring 2021
---

### About

The methods used in deriving a predictive model and later evaluating performance form the building blocks to any work in machine learning. The first step is to split a given set of data into two separate data sets: a training set and a testing set, with the training set greater than or equal to 50% of the data. In principle the bigger the training set the better the model derived. We use the training set to build a model best predictor of the data. Later, we test the built model on the testing data set, and assess how good the model is in predicting the actual values.  This approach is common in supervised machine learning where the output of a model can be assessed against expected outcome.

The concept is not very complicated but it is important to understand the steps involved in the training stage and differentiate from the steps in the testing stage.  There is no model derivation in the testing stage, only evaluation.  

### Note

Read carefully the instructions on Sakai. For clarity, questions are highlighted in red color and numbered according to their particular placement in the task section. Quite often you may need to add your own code chunk.

Execute all code chunks, save your work, preview, and submit your final $\it lab10.nb.html$ file in Sakai.

--------------

### Task 1: Model Building using a Training Set (total 6pts)

The first half of this lab focuses on building a model using the training data. The data we will be using was obtained from Kaggle site[1] and is in reference to world university rankings [2] . The data looks at university world scoring based on different ranking criteria such as quality of education, quality of faculty, and rank for patents. Spend some time to visit the referenced website to get more acquinated with the data.  The data obtained is divided inteo two sets: a training set and a testing set. We begin by reading the training data set 'universityrank_training.csv' file, and checking the header lines to make sure the data is read correctly. 

```{r}
traindata = read.csv(file="universityrank_training.csv", header=TRUE)
head(traindata)
```

Next, we extract the two columns of interest and call them properly so we can easily refer to them later in the code. 

```{r}
patent_train = traindata$patents
score_train = traindata$score
```

The first model we will build is a simple linear model. We will use the patents ranking variable to predict the university score.  To better understand the data, the lower the patents ranking number the better it is.  A value of 1 is a top rank for patents and represent the highest category in terms of number of patents owned by the particular academic institution. On the other hand the higher the calculated total score the better, as reflected  by the world rank number. A  value of 100 is a perfect score.  

```{r}
linear_train = lm(score_train ~ patent_train)
summary(linear_train)

plot(patent_train,score_train)
abline(linear_train, col="blue", lwd=2)
```

<span style="color:red">
##### 1A) Complete the code chunk below to build a non-linear quadratic model. Follow the steps in $lab08$ to derive a quadratic model for costs versus servers (4pts)
</span>

```{r}
# First define a new variable which is the squared value of patent_train (introduced above)
patent_train_squared = patent_train ^ 2

# Next derive the quadratic regression model.  You may want to call it quad_train.
quad_train = lm(score_train ~ patent_train + patent_train_squared)

# Publish the summary statistics
summary(quad_train)
```

<span style="color:red">
##### 1B) Looking at the summary statistics for both the linear and the quadratic select the best predictive model. Explain your logic (2pts)
</span>

<span style="color:blue">
The quadratic model would be a better predictive model because its adjusted R-squared value of `0.3453` is greater the linear model's value of `0.2314`.
</span>

----------

### Task 2: Model Evaluation using a Testing Set (total 14pts)

The second half of predictive modeling is about evaluating the model using a different data set called the testing data. Again we must first read the testing data set, and make sure the dataset is read propertly.

```{r}
testdata = read.csv("universityrank_testing.csv", header=TRUE)
head(testdata)
```

We extract again the two columns of interest, in reference this time to the testing data set, and call them accordingly so as not to confuse with the training set.

```{r}
patent_test = testdata$patents
score_test = testdata$score
```

We are ready now to check if the derived models are actually good predictive models. First we calculate the predicted test data score using the linear model as derived from the training set. Later we will consider the quadratic model. 

```{r}
# Calculate the predicted test data score 
score_predict1 = coef(linear_train)[1] + coef(linear_train)[2]*patent_test
```

For a visual qualitative evaluation we can plot the actual testing data, and overlay the predicted values obtained from linear model.

```{r}
# Plot the actual values for patent and score as observed in the testing data set
plot(patent_test, score_test, main='Test Data -- Score vs Patent')
# Overlay the predicted values as calculated from the linear model and derived using the training model
par(new=TRUE, xaxt="n", yaxt="n", ann=FALSE)
# The red color is used to distinguish the predicted values which, because of the linear model, will fit exactly a line
plot(patent_test, score_predict1, col="red")
```

A better way to qualify the goodness of a predictive model is to create a scatter plot of the actual/observed values against the predicted values. In a perfect predictive model the points will line up along the diagonal line.  This is rarely the case, if ever!

```{r}
#Plot predicted values from the linear model versus actual values form the test data
plot(score_test, score_predict1, xlab='Actual', ylab='Predict', main='Linear Model -- Predict vs Actual Test')
```

From the plot we can easily see that most of the predicted values versus the actual values are far from the diagonal line, suggesting a poor prediction. In many cases this is fine. Finally, to quantify the goodness of a  model, we need to calculate the Root Mean Square Error (RMSE).

```{r}
#Calculate RMSE for Linear Model
error1 = sum((score_predict1 - score_test)^2)/length(score_test)
rmse1 = sqrt(error1)
rmse1
```


It is hard to judge the goodness of the number unless we compare to other possibilities. Of course a perfect scenario will have zero RMSE. We now need to repeat the above calculations for the non-linear quadratic model.

<span style="color:red">
##### 2A) Fill in the code chunk below to calculate the predicted values for the non-linear quadratic model (4pts)
</span>

```{r}
# Calculate the predicted score_predict2 using the non-linear quadratic model as applied to the actual patent values in the testing data
score_predict2 = coef(quad_train)[1] + coef(quad_train)[2]*patent_test + coef(quad_train)[3]*(patent_test ^ 2)
```

For a visual representation, similar to the linear model, we need to do the following.

<span style="color:red">
##### 2B) Fill-in the code chunk below to plot the actual score versus patent for the test data, and overlay the predicted values as calculated in 2A. Label axes and title properly (4pts)
</span>

```{r}
# Plot the actual values for patent and score as observed in the testing data set
plot(patent_test, score_test, main='Test Data -- Score vs Patent')

# Overlay the predicted values score_predict2 as calculated from the quadratic model using the training model. 
par(new=TRUE, xaxt="n", yaxt="n", ann=FALSE)

# The green color is used to distinguish the predicted values which, because of the quadratic model, will in this case will fit exactly a parabola
plot(patent_test, score_predict2, col="green")
```

<span style="color:red">
#### 2C) Fill-in the code chunk below to plot the Predict vs Actual for the quadratic model.  Label axes and title properly (2pts)
</span>

```{r}
#Plot the predicted values form the quadratic model versus the actual values from the test data
plot(score_test, score_predict2, xlab='Actual', ylab='Predict', main='Quadratic Model -- Predict vs Actual Test')
```

<span style="color:red">
#### 2D) By looking at the scatterplots of predict versus actual for both the linear and quadratic models are you able to tell which model is better? Explain your logic (1pt)
</span>

<span style="color:blue">
By simply looking at the scatterplots themselves, it is difficult to distiguish which model would be better than the other because neither scatterplot has a clear indication of predicted values vs. actual values along the overlaid predicted values from both models. If I were to have to choose one, I would choose the quadratic model because there are a number of plots that line up with the vertex of the plotted parabola.
</span>

A better way is to quantify the goodness of the model by calculating again the RMSE.

<span style="color:red">
#### 2E) Calculate the root mean square error (RMSE) for the quadratic model (2pts)
</span>

```{r}
#Calculate RMSE for Quadratic Model
error2 = sum((score_predict2 - score_test)^2)/length(score_test)
rmse2 = sqrt(error2)
rmse2
```

<span style="color:red">
#### 2F) Compare the root mean square error (RMSE) for both the linear and quadratic models. Which model is better? How your conclusion reconcile with the results from Task 1? (1pt)
</span>

<span style="color:blue">
Based on the calculated RMSEs, the quadratic model would be a better model because its RMSE value of `5.685396` is closer to `0` than the linear model's RMSE value of `5.95868`. This conclusion is supported by the results from Task 1.
</span>

source [1]: [http://www.kaggle.com](http://www.kaggle.com)

source [2]: [http://www.cwur.org](http://www.cwur.org) 
